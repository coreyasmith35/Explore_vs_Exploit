# Explore vs Exploit Problem

### Algorithms:
- Random Selection
- Epsilon Greedy
- Upper Confidence Bound (UBC)
- Thompson Sampling

### About the problem:
The problem of exploration vs exploit is an optimization problem. When is it best to learn and when is it best to take action on what we have learned.

Often this problem is represented by Multi-armed bandits but these algorithms can be used to solve many
optimization problems such as scheduling, advertisement, smarter A/B testing.

This repo looks at the problem using advertising.

### About the data:
The data represents 10 different advertisements run over 10000 different times.

|Ad 1|Ad 2|Ad 3|Ad 4|Ad 5|Ad 6|Ad 7|Ad 8|Ad 9|Ad 10|
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|1|0|0|0|1|0|0|0|1|0|
|0|0|0|0|0|0|0|0|1|0|
|0|0|0|0|0|0|0|0|0|0|

Each row represents a different ad rotation or day an ad was run, we are to select one ad to run during each rotation. Depending on the ad we choose we will receive a reward of:

    - 1 represents a successful ad
    - 0 represents an unsuccessful ad

### Goal:

Find the most successful ad while maximizing our reward over the 10000 rotations/days.

#### Random Selection:

This algorithm serves a baseline to compare all others. The agent
chooses an ad to run at random.

![alt text][random_hist] ![alt text][random_reward] ![alt text][random_ads]

    Random Selection - Average total reward: 0.1248

[random_hist]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/random_hist.png?raw=true "Random - histogram of ads selections"

[random_reward]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/random_reward.png?raw=true "Random -Running average Rewards"

[random_ads]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/random_ads.png?raw=true "Random -Number of ads run"

#### Epsilon Greedy:

This algorithm is based on a probability of exploration. The agent has a set likely hood Îµ that it will explore. This algorithm can be improved by implementing one of the many alternative algorithms such as a decaying epsilon, optimistic initial values, Contextual-Epsilon-greedy and many more. But here we will look at the most simplest form of epsilon-greedy.

![alt text][eps_hist] ![alt text][eps_reward] ![alt text][eps_ads]

###### Comparing epsilon Values -

![alt text][eps_comparing]

    Epsilon Greedy( 0.05 ) - Average total reward: 0.2589
    Epsilon Greedy( 0.25 ) - Average total reward: 0.2189
    Epsilon Greedy( 0.5 ) - Average total reward: 0.1937

[eps_hist]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/eps_hist.png?raw=true "eps - histogram of ads selections"

[eps_reward]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/eps_reward.png?raw=true "eps -Running average Rewards"

[eps_ads]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/eps_ads.png?raw=true "Eps -Number of ads run"

[eps_comparing]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/comparing.png?raw=true "Eps -Comparing epsilon values"


#### Upper Confidence Bound (UBC):

This algorithm chooses actions as if the environment is the best it can be. In the case of the ads, this algorithm will choose an advertisement best expected outcome in a know interval. Each ad has a confidence interval that shrinks as more information about the ad is presented. The agent selects the ad with the highest upper bound on this interval.

![alt text][ubc_hist] ![alt text][ubc_reward] ![alt text][ubc_ads]

    Upper Confidence Bound - Average total reward: 0.2178

[ubc_hist]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/ucb_hist.png?raw=true "UBC - histogram of ads selections"

[ubc_reward]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/ucb_reward.png?raw=true "UBC -Running average Rewards"

[ubc_ads]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/ucb_ads.png?raw=true "UBC -Number of ads run"


#### Thompson Sampling:

This algorithm is a natural Bayesian heuristic. The agent maintains a belief or distribution for the unknown likelihood that an ad will be successful. Each time an ad is run the agent updates its belief in a Bayesian manner. The agent selects an ad with the probability of being the best ad.

![alt text][ts_hist] ![alt text][ts_reward] ![alt text][ts_ads]

    Thompson Sampling - Average total reward: 0.2585

[ts_hist]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/ts_hist.png?raw=true "TS - histogram of ads selections"

[ts_reward]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/ts_reward.png?raw=true "TS -Running average Rewards"

[ts_ads]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/ts_ads.png?raw=true "TS -Number of ads run"


### Comparing the Different Algorithms:

In this application the Epsilon Greedy and Thompson Sampling where our winners and followed closely by Upper Confidence Bound. Note that even thought these algorithms preformed very close to each other, in other applications there could be a undisputed better algorithm.

![alt text][compare]

[compare]: https://github.com/coreyasmith35/Explore_vs_Exploit/blob/master/Graphs/comparing.png?raw=true "Comparing the algos"
